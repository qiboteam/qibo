{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double-bracket Iteration other cost functions and respective scheduling\n",
    "\n",
    "This notebook presents two additional cost functions for the double-bracket flow: least-squares and energy fluctuation with their respectice scheduling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qibo import hamiltonians, set_backend\n",
    "from qibo.models.dbi.double_bracket import DoubleBracketGeneratorType, DoubleBracketScheduling, DoubleBracketIteration, DoubleBracketCostFunction\n",
    "from qibo.models.dbi.utils import *\n",
    "from qibo.models.dbi.utils_scheduling import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-squares\n",
    "\n",
    "The cost function is defined as: $\\frac{1}{2}||D-H_k||^2 =\\frac{1}{2}(||D||^2+||H||^2) -Tr(D H_k)$ as in (the negative of https://epubs.siam.org/doi/abs/10.1137/S0036141092229732?journalCode=sjmael) We seek to minimize this function at each DBF iteration. For numerical optimizations, we also ignore the norm of H term as for a given hamiltonian it is fixed through out the flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamiltonian\n",
    "set_backend(\"numpy\")\n",
    "\n",
    "# hamiltonian parameters\n",
    "nqubits = 5\n",
    "h = 3.0\n",
    "\n",
    "# define the hamiltonian\n",
    "H_TFIM = hamiltonians.TFIM(nqubits=nqubits, h=h)\n",
    "\n",
    "# define the least-squares cost function\n",
    "cost = DoubleBracketCostFunction.least_squares\n",
    "# initialize class\n",
    "dbi = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for plotting sigma decrease of the first step\n",
    "d = np.diag(np.linspace(1,2**nqubits,2**nqubits))/2**nqubits\n",
    "s_space = np.linspace(1e-5, 1.0, 500)\n",
    "off_diagonal_norm_diff = []\n",
    "potential = []\n",
    "for s in s_space:\n",
    "    dbi_eval = deepcopy(dbi)\n",
    "    dbi_eval(s,d=d)\n",
    "    off_diagonal_norm_diff.append(dbi_eval.off_diagonal_norm - dbi.off_diagonal_norm)\n",
    "    potential.append(dbi_eval.least_squares(d=d))\n",
    "\n",
    "# grid_search\n",
    "step_grid = dbi.choose_step(scheduling=DoubleBracketScheduling.grid_search,d=d)\n",
    "print('grid_search step:', step_grid)\n",
    "# hyperopt\n",
    "step_hyperopt = dbi.choose_step(scheduling=DoubleBracketScheduling.hyperopt,d=d, max_evals=100, step_max=0.6)\n",
    "print('hyperopt_search step:', step_hyperopt)\n",
    "# polynomial\n",
    "step_poly = dbi.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation,d=d, n=3)\n",
    "print('polynomial_approximation step:', step_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.plot(s_space, potential)\n",
    "plt.xlabel('s')\n",
    "plt.axvline(x=step_grid, color='r', linestyle='-',label='grid_search')\n",
    "plt.axvline(x=step_hyperopt, color='g', linestyle='--',label='hyperopt')\n",
    "plt.axvline(x=step_poly, color='m', linestyle='-.',label='polynomial')\n",
    "plt.title('First DBI step')\n",
    "plt.ylabel('Least squares cost function')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(s_space, off_diagonal_norm_diff)\n",
    "plt.axvline(x=step_grid, color='r', linestyle='-',label='grid_search')\n",
    "plt.axvline(x=step_hyperopt, color='g', linestyle='--',label='hyperopt')\n",
    "plt.axvline(x=step_poly, color='m', linestyle='-.',label='polynomial')\n",
    "plt.ylabel(r'$||\\sigma(H_0)||-\\sigma(H_k)||$')\n",
    "plt.xlabel('s')\n",
    "plt.title('First DBI step')\n",
    "plt.legend()\n",
    "print('The minimum for cost function in the tested range is:', step_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of the least-squares cost function with the original cost function using the polynomial scheduling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diag(np.linspace(1,2**nqubits,2**nqubits))\n",
    "off_diagonal_norm_diff = [dbi.off_diagonal_norm]\n",
    "off_diagonal_norm_diff_least_squares = [dbi.off_diagonal_norm]\n",
    "iters = 100\n",
    "dbi_ls = deepcopy(dbi)\n",
    "cost = DoubleBracketCostFunction.off_diagonal_norm\n",
    "dbi_od = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost)\n",
    "for _ in range(iters):\n",
    "    step_poly = dbi_od.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation, d=d, n=3)\n",
    "    dbi_od(step_poly,d=d)\n",
    "    step_poly = dbi_ls.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation, d=d, n=3)\n",
    "    dbi_ls(step_poly,d=d)\n",
    "    off_diagonal_norm_diff.append(dbi_od.off_diagonal_norm)\n",
    "    off_diagonal_norm_diff_least_squares.append(dbi_ls.off_diagonal_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(iters+1), off_diagonal_norm_diff, label=r'Off-diagonal norm')\n",
    "plt.plot(range(iters+1), off_diagonal_norm_diff_least_squares, label=r'Least squares')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel(r'$||\\sigma(H_k)||$')\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy fluctuation\n",
    "\n",
    "This cost function is defined as: $\\Xi_k^2 (\\mu) = \\langle \\mu | H_k^2| \\mu \\rangle - \\langle \\mu | H_k| \\mu \\rangle^2$. We must specify the state $| \\mu \\rangle$ for which we want to minimize the fluctuation. The overall diagonalization isn't guaranteed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamiltonian\n",
    "set_backend(\"numpy\")\n",
    "\n",
    "# hamiltonian parameters\n",
    "nqubits = 3\n",
    "h = 3.0\n",
    "\n",
    "# define the hamiltonian\n",
    "H_TFIM = hamiltonians.TFIM(nqubits=nqubits, h=h)\n",
    "\n",
    "# define the energy fluctuation cost function\n",
    "cost = DoubleBracketCostFunction.energy_fluctuation\n",
    "# define the state\n",
    "state = np.zeros(2**nqubits)\n",
    "state[3] = 1\n",
    "# initialize class\n",
    "dbi = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost, ref_state=state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for plotting sigma decrease of the first step\n",
    "d = np.diag(np.linspace(2**nqubits,1,2**nqubits))/2**nqubits\n",
    "s_space = np.linspace(-1, 1, 1000)\n",
    "off_diagonal_norm_diff = []\n",
    "fluctuation = []\n",
    "for s in s_space:\n",
    "    dbi_eval = deepcopy(dbi)\n",
    "    dbi_eval(s,d=d)\n",
    "    off_diagonal_norm_diff.append(dbi_eval.off_diagonal_norm - dbi.off_diagonal_norm)\n",
    "    fluctuation.append(dbi_eval.energy_fluctuation(state=state))\n",
    "\n",
    "# grid_search\n",
    "step_grid = dbi.choose_step(scheduling=DoubleBracketScheduling.grid_search,d=d)\n",
    "print('grid_search step:', step_grid)\n",
    "# hyperopt\n",
    "step_hyperopt = dbi.choose_step(scheduling=DoubleBracketScheduling.hyperopt,d=d, max_evals=100, step_max=0.6)\n",
    "print('hyperopt_search step:', step_hyperopt)\n",
    "# polynomial\n",
    "step_poly = dbi.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation,d=d, n=3)\n",
    "print('polynomial_approximation step:', step_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.plot(s_space, fluctuation)\n",
    "plt.xlabel('s')\n",
    "plt.axvline(x=step_grid, color='r', linestyle='-',label='grid_search')\n",
    "plt.axvline(x=step_hyperopt, color='g', linestyle='--',label ='hyperopt')\n",
    "plt.axvline(x=step_poly, color='m', linestyle='-.',label='polynomial')\n",
    "plt.title('First DBI step')\n",
    "plt.ylabel('Energy fluctuation')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(s_space, off_diagonal_norm_diff)\n",
    "plt.axvline(x=step_grid, color='r', linestyle='-',label='grid_search')\n",
    "plt.axvline(x=step_hyperopt, color='g', linestyle='--',label='hyperopt')\n",
    "plt.axvline(x=step_poly, color='m', linestyle='-.',label='polynomial')\n",
    "plt.ylabel(r'$||\\sigma(H_0)||-\\sigma(H_k)||$')\n",
    "plt.xlabel('s')\n",
    "plt.title('First DBI step')\n",
    "plt.legend()\n",
    "print('The minimum for cost function in the tested range is:', step_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diag(np.linspace(1,2**nqubits,2**nqubits))\n",
    "off_diagonal_norm_diff = [dbi.off_diagonal_norm]\n",
    "energy_fluc = [dbi.energy_fluctuation(state=state)]\n",
    "iters = 10\n",
    "dbi_ = deepcopy(dbi)\n",
    "for _ in range(iters):\n",
    "    step_poly = dbi_.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation, d=d, n=3)\n",
    "    dbi_(step_poly,d=d)\n",
    "    off_diagonal_norm_diff.append(dbi_.off_diagonal_norm)\n",
    "    energy_fluc.append(dbi_.energy_fluctuation(state=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(iters+1), off_diagonal_norm_diff)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel(r'$||\\sigma(H_k)||$')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(iters+1), energy_fluc)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel(r'Energy fluctuation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 30\n",
    "states = [0,1,2,3,4,5,6,7]\n",
    "energy = np.empty((len(states),iters))\n",
    "\n",
    "\n",
    "d = (np.diag(np.linspace(1,2**nqubits,2**nqubits)))\n",
    "for i in range(len(states)):\n",
    "    dbi_ = deepcopy(dbi)\n",
    "    dbi_.state = states[i]\n",
    "    for j in range(iters):\n",
    "        step_poly = dbi_.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation, d=d, n=3)\n",
    "        if step_poly is not None:\n",
    "            dbi_(step_poly, d=d)\n",
    "        energy[i,j] = np.real(dbi_.h.matrix[states[i],states[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals = np.linalg.eigh(dbi_.h.matrix)[0]\n",
    "print('Eigenvalues:', eigvals )\n",
    "plt.figure()\n",
    "for i in range(len(states)):\n",
    "    plt.plot(range(iters), energy[i,:],'.', label='State ' + str(states[i]))\n",
    "for eigvals in eigvals:\n",
    "    plt.axhline(y=eigvals, color='r', linestyle='--')\n",
    "plt.xlabel('Iterations')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for finding optimal $D$\n",
    "\n",
    "An advantage of the least-squares cost function is that one can use gradient descent and the learning is more stable than with the off-diagonal cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qibo.models.dbi.utils_gradients import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamiltonian\n",
    "set_backend(\"numpy\")\n",
    "\n",
    "# hamiltonian parameters\n",
    "nqubits = 3\n",
    "h = 3.0\n",
    "\n",
    "# define the hamiltonian\n",
    "H_TFIM = hamiltonians.TFIM(nqubits=nqubits, h=h)\n",
    "\n",
    "# define the energy fluctuation cost function\n",
    "cost = DoubleBracketCostFunction.energy_fluctuation\n",
    "# define the state\n",
    "state = np.zeros(2**nqubits)\n",
    "state[3] = 1\n",
    "# initialize class\n",
    "dbi = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost, ref_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = DoubleBracketCostFunction.least_squares\n",
    "nqubits = 5\n",
    "H_TFIM = hamiltonians.TFIM(nqubits=nqubits, h=h)\n",
    "dbi = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost)\n",
    "params = np.linspace(1,2**nqubits,2**nqubits)\n",
    "\n",
    "step = 1e-2\n",
    "iterations = 200\n",
    "d, loss, grad, diags = gradient_descent_dbr_d_ansatz(dbi, params, iterations, step)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(iterations+1), loss)\n",
    "plt.xlabel('Learning iterations')\n",
    "plt.ylabel('Loss: Least squares')\n",
    "\n",
    "plt.figure()\n",
    "for i in range(2**nqubits):\n",
    "    plt.plot(diags[i,:], label='State ' + str(i))\n",
    "plt.xlabel('Learning iterations')\n",
    "plt.ylabel('Diagonal elements')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for $D$ can greatly improve the decrease of the off-diagonal norm at each iteration. Nonetheless, during training the ascending values condition may be no longer satisfied creating a exponential decrease after few iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = DoubleBracketCost.least_squares\n",
    "nqubits = 5\n",
    "H_TFIM = hamiltonians.TFIM(nqubits=nqubits, h=3.0)\n",
    "dbi = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost)\n",
    "params = np.linspace(1,2**nqubits,2**nqubits)\n",
    "d_fixed = np.diag(params)\n",
    "dbi_trained = deepcopy(dbi)\n",
    "flows = 50\n",
    "iterations = 200\n",
    "off_diagonal_norm = np.empty((2,flows+1))\n",
    "off_diagonal_norm[0,0] = dbi_trained.off_diagonal_norm\n",
    "off_diagonal_norm[1,0] = dbi.off_diagonal_norm\n",
    "d_trained, loss, grad, diags = gradient_descent_dbr_d_ansatz(dbi_trained, params, iterations,step)\n",
    "for i in range(flows):\n",
    "\n",
    "    s = dbi_trained.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation, d=d_trained, n=3)\n",
    "    dbi_trained(s,d=d_trained)\n",
    "    off_diagonal_norm[0,i+1] = dbi_trained.off_diagonal_norm\n",
    "    s = dbi.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation, d=d_fixed, n=3)\n",
    "    dbi(s,d=d_fixed)\n",
    "    off_diagonal_norm[1,i+1] = dbi.off_diagonal_norm\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(off_diagonal_norm[0,:], label='Trained')\n",
    "plt.plot(off_diagonal_norm[1,:], label='Untrained')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel(r'$||\\sigma(H_k)||$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solution can be to redo the training at each step, with a $D$ having ascending values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = DoubleBracketCost.least_squares\n",
    "nqubits = 5\n",
    "H_TFIM = hamiltonians.TFIM(nqubits=nqubits, h=3.0)\n",
    "dbi = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost)\n",
    "params = np.linspace(1,2**nqubits,2**nqubits)\n",
    "d_fixed = np.diag(params)\n",
    "dbi_trained = deepcopy(dbi)\n",
    "flows = 50\n",
    "iterations = 20\n",
    "off_diagonal_norm = np.empty((2,flows+1))\n",
    "off_diagonal_norm[0,0] = dbi_trained.off_diagonal_norm\n",
    "off_diagonal_norm[1,0] = dbi.off_diagonal_norm\n",
    "\n",
    "for i in range(flows):\n",
    "    d_trained, loss, grad, diags = gradient_descent_dbr_d_ansatz(dbi_trained, params, iterations,step)\n",
    "    s = dbi_trained.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation, d=d_trained, n=3)\n",
    "    dbi_trained(s,d=d_trained)\n",
    "    off_diagonal_norm[0,i+1] = dbi_trained.off_diagonal_norm\n",
    "    s = dbi.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation, d=d_fixed, n=3)\n",
    "    dbi(s,d=d_fixed)\n",
    "    off_diagonal_norm[1,i+1] = dbi.off_diagonal_norm\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(off_diagonal_norm[0,:], label='Trained')\n",
    "plt.plot(off_diagonal_norm[1,:], label='Untrained')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel(r'$||\\sigma(H_k)||$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical gradients may be preferred as they decrease more the loss at each iteration and are computationally faster. They may be more precise as the previous analytic since the analytic computations use the polynomial approximation as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nqubits = [3,4,5,6]\n",
    "iterations = 30\n",
    "step = 1e-2\n",
    "differences = np.empty((len(nqubits),iterations+1))\n",
    "loss_max = np.empty(len(nqubits))\n",
    "for q in range(len(nqubits)):\n",
    "    # define the hamiltonian\n",
    "    H_TFIM = hamiltonians.TFIM(nqubits=nqubits[q], h=h)\n",
    "\n",
    "    # define the least-squares cost function\n",
    "    cost = DoubleBracketCost.least_squares\n",
    "    # initialize class\n",
    "    dbi = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost)\n",
    "    loss_max [q] = dbi.least_squares(d=np.diag(np.linspace(1,2**nqubits[q],2**nqubits[q])))\n",
    "    params = np.linspace(1,2**nqubits[q],2**nqubits[q])\n",
    "    d_analytic, loss_analytic, grad_analytic, diags_analytic = gradient_descent_dbr_d_ansatz(dbi, params, iterations, step)\n",
    "    params = np.linspace(1,2**nqubits[q],2**nqubits[q])\n",
    "    d_numerical, loss_numerical, grad_numerical, diags_numerical = gradient_descent_dbr_d_ansatz(dbi, params,iterations,step, analytic=False)\n",
    "    differences[q,:] = loss_analytic - loss_numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for q in range(len(nqubits)):\n",
    "    plt.plot(differences[q,:],label= 'nqubits = {}'.format(nqubits[q]))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Difference in analytic and numerical loss function')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Normalized difference')\n",
    "for q in range(len(nqubits)):\n",
    "    plt.plot(differences[q,:]/loss_max[q],label= 'nqubits = {}'.format(nqubits[q]))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Difference in analytic and numerical loss function')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-local ansatz\n",
    "\n",
    "We can consider, as an alternative to the a fully parametrized diagonal, a diagonal matrix of the form: $D = \\sum \\alpha_i Z_i$. This has the advantage of having a linear number of parameters to optimize instead of an exponential as well as being easier to implement in a quantum computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamiltonian\n",
    "set_backend(\"numpy\")\n",
    "\n",
    "# hamiltonian parameters\n",
    "nqubits = 5\n",
    "h = 3.0\n",
    "\n",
    "# define the hamiltonian\n",
    "H_TFIM = hamiltonians.TFIM(nqubits=nqubits, h=h)\n",
    "\n",
    "# define the least-squares cost function\n",
    "cost = DoubleBracketCost.least_squares\n",
    "\n",
    "# initialize class\n",
    "dbi = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_eval = deepcopy(dbi)\n",
    "params = np.linspace(1,2**nqubits,2**nqubits)\n",
    "d_opt, loss_opt, grad_opt, diags_opt = gradient_descent_dbr_d_ansatz(dbi, params,  100,1e-2, analytic=False, d_type = d_ansatz_type.element_wise)\n",
    "flows = 50\n",
    "off_diagonal_norm = np.empty((flows+1,2))\n",
    "off_diagonal_norm[0,:] = dbi_eval.off_diagonal_norm\n",
    "for i in range(flows):\n",
    "    step_poly = dbi_eval.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation,d=d_opt,n=3)\n",
    "    dbi_eval(step_poly,d=d_opt)\n",
    "    off_diagonal_norm[i+1,0] = dbi_eval.off_diagonal_norm\n",
    "\n",
    "\n",
    "\n",
    "dbi_eval = deepcopy(dbi)\n",
    "params = np.linspace(1,nqubits,nqubits)\n",
    "d_opt, loss_opt, grad_opt, diags_opt = gradient_descent_dbr_d_ansatz(dbi, params,  30, 1e-3, analytic=False, d_type = d_ansatz_type.local_1)\n",
    "best = np.argmin(loss_opt)\n",
    "d_opt = d_ansatz(diags_opt[:,best], d_ansatz_type.local_1)\n",
    "for i in range(flows):\n",
    "    step_poly = dbi_eval.choose_step(scheduling=DoubleBracketScheduling.polynomial_approximation,d=d_opt,n=3)\n",
    "    dbi_eval(step_poly,d=d_opt)\n",
    "    off_diagonal_norm[i+1,1] = dbi_eval.off_diagonal_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(off_diagonal_norm[:,0],label='element-wise ansatz')\n",
    "plt.plot(off_diagonal_norm[:,1],label='1-local ansatz')\n",
    "plt.xlabel('Flows Iterations')\n",
    "plt.ylabel(r'$||\\sigma(H_k)||$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamiltonian\n",
    "set_backend(\"numpy\")\n",
    "\n",
    "# hamiltonian parameters\n",
    "nqubits = 5\n",
    "h = 3.0\n",
    "\n",
    "# define the hamiltonian\n",
    "H_TFIM = hamiltonians.TFIM(nqubits=nqubits, h=h)\n",
    "\n",
    "# define the least-squares cost function\n",
    "cost = DoubleBracketCost.least_squares\n",
    "\n",
    "# initialize class\n",
    "dbi = DoubleBracketIteration(deepcopy(H_TFIM),mode=DoubleBracketGeneratorType.single_commutator,cost=cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_eval = deepcopy(dbi)\n",
    "params = np.linspace(1,nqubits,nqubits)\n",
    "d_opt, loss_opt, grad_opt, diags_opt = gradient_descent_dbr_d_ansatz(dbi, params,  20, 1e-3, analytic=False, d_type = d_ansatz_type.local_1)\n",
    "best = np.argmin(loss_opt)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_opt)\n",
    "\n",
    "s = np.linspace(-0.1,0.1,100)\n",
    "least_squares = np.empty(100)\n",
    "off_diagonal_norm = np.empty(100)\n",
    "for i in range(100):\n",
    "    dbi_eval(s[i],d=d_opt)\n",
    "    least_squares[i] = dbi_eval.least_squares(d=d_opt)\n",
    "    off_diagonal_norm[i] = dbi_eval.off_diagonal_norm\n",
    "plt.figure()\n",
    "plt.plot(s,loss)\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('Least squares cost function')\n",
    "plt.figure()\n",
    "plt.plot(s,off_diagonal_norm)\n",
    "plt.xlabel('s')\n",
    "plt.ylabel(r'$||\\sigma(H_k)||$')\n",
    "\n",
    "\n",
    "print(np.diag(d_opt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43f1f904380137ff38e17e8a93371c4872e6bababc18e270d8a0497ea5c7ea38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
